{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iriyagupta/GENAI-BA-CPlus/blob/main/Text_Generation_with_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Logo](https://plus.columbia.edu/sites/default/files/logo/columbiapluslogo_3_30_2.png)"
      ],
      "metadata": {
        "id": "WjJNkiwvSmAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning objective of this notebook\n",
        "\n",
        "### 1. General approach to NLP tasks\n",
        "* data cleaning (lower case, remove punctuation, etc)\n",
        "* tokenize\n",
        "* n-gram sequence\n",
        "\n",
        "### 2. High level understanding of LSTM\n",
        "* a type of RNN\n",
        "* advantage over traditional RNN -- ability to learn long term dependencies\n",
        "\n",
        "### 3. Best practices for training a Neural Network\n",
        "* regularization layer to prevent over fitting\n",
        "* use checkpoint to prevent loss of data\n",
        "* save model and restore when needed\n"
      ],
      "metadata": {
        "id": "p0ET6J-zgbde"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlOipocjJVfU"
      },
      "source": [
        "## How to use Colab with Google Drive\n",
        "\n",
        "You might want to mount your Google Drive to Colab so that you can access a data file from your drive or save a trained model to your drive.\n",
        "\n",
        "You can mount your Google Drive to Colab by running the cell below and follow the instructions in the popup window.\n",
        "\n",
        "TODO: maybe create a brief \"intro to colab\" section in the first notebook? First as in the first time we use a colab notebook in the course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlKalaAd_Yp8",
        "outputId": "41b80933-292b-4095-8ca9-47b7b3892155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Mk_a_PlgSWeo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKvJbcTfFs0j"
      },
      "source": [
        "# Text Generation with LSTMs\n",
        "\n",
        "## Write like Jane Austen\n",
        "\n",
        "In this notebook, we will be using Pride and Prejudice by Jane Austen to train a Neural Network that can write like Jane Austen. Isn't it cool!?\n",
        "\n",
        "\n",
        "## Import the libraries\n",
        "\n",
        "As the first step, we need to import the required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h995w-X6Fs0k"
      },
      "outputs": [],
      "source": [
        "# keras module for building LSTM\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "\n",
        "# set seeds for reproducability\n",
        "import tensorflow as tf\n",
        "from numpy.random import seed\n",
        "tf.random.set_seed(1221)\n",
        "seed(404)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCGvTLdGFs0l"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "Load the text of Pride and Prejudice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zkOotXOFs0l"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "path_to_file = \"/content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/pride_and_prejudice.txt\"\n",
        "with open(path_to_file, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9v3T5DXiU6H"
      },
      "source": [
        "Perform some initial data cleaning such as converting to lower case and removing punctuation, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js_FgO0bh7HP",
        "outputId": "8ca5dc32-61e3-49c2-cb67-b8ee93722e08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pride and prejudice\n",
            "\n",
            "by jane austen\n",
            "\n",
            "chapter 1\n",
            "\n",
            "it is a truth universally acknowledged that a single man in possession\n",
            "of a good fortune must be in want of a wife\n",
            "\n",
            "however little known the feelings or views of such a man may be on his\n"
          ]
        }
      ],
      "source": [
        "# Convert text to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Remove punctuation\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "text = text.translate(translator)\n",
        "\n",
        "# Tokenization\n",
        "# The Tokenizer from Keras will be used later to tokenize the text\n",
        "corpus = text.split(\"\\n\")  # Splits the text into lines\n",
        "\n",
        "# Display some processed lines\n",
        "for i in range(10):\n",
        "    print(corpus[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guZfJ_GFFs0l"
      },
      "source": [
        "## Dataset preparation\n",
        "* Human Readability: Humans can easily read and understand text because we have the ability to comprehend language, context, emotions, and cultural nuances. We interpret words, sentences, and their meanings based on our knowledge and experience.\n",
        "\n",
        "* Machine Comprehension: Machines, on the other hand, do not inherently understand text in the same way. Computers process data numerically, so text data must be converted into a format that can be represented numerically. This is where tokenization and further processing come into play.\n",
        "\n",
        "\n",
        "### Tokenize the text\n",
        "Tokenization is a process of extracting tokens (terms / words) from a corpus. The `tf.keras.preprocessing.text.Tokenizer` layer can convert each word into a numeric index in the corpus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB0_Rl9Vk8Rj"
      },
      "source": [
        "In this example below, we demonstrate how a tokenizer convert words into numbers and how to recover a sentence from sequence of numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1YepaFpjRKR"
      },
      "outputs": [],
      "source": [
        "sample_text = \"Columbia Plus is amazing\"\n",
        "tokenizer = Tokenizer()\n",
        "corpus = sample_text.split(\" \")\n",
        "tokenizer.fit_on_texts(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bl1f-t0MKLXw",
        "outputId": "e38e6532-c9de-4c2d-b1de-ef85054f6d6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Columbia', 'Plus', 'is', 'amazing']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewPlGzonlcM1"
      },
      "source": [
        "From the below `word_index`, we see that each word in out sample text has been assigned an index and we can recover a sentence from a sequence of numbers by refering to those indicies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdhEgl-yjcRZ",
        "outputId": "60a43733-62e3-4d46-8fcd-8a06c473d04a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_words': None,\n",
              " 'filters': '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
              " 'lower': True,\n",
              " 'split': ' ',\n",
              " 'char_level': False,\n",
              " 'oov_token': None,\n",
              " 'document_count': 4,\n",
              " 'word_counts': '{\"columbia\": 1, \"plus\": 1, \"is\": 1, \"amazing\": 1}',\n",
              " 'word_docs': '{\"columbia\": 1, \"plus\": 1, \"is\": 1, \"amazing\": 1}',\n",
              " 'index_docs': '{\"1\": 1, \"2\": 1, \"3\": 1, \"4\": 1}',\n",
              " 'index_word': '{\"1\": \"columbia\", \"2\": \"plus\", \"3\": \"is\", \"4\": \"amazing\"}',\n",
              " 'word_index': '{\"columbia\": 1, \"plus\": 2, \"is\": 3, \"amazing\": 4}'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "tokenizer.get_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ3Zn5nkl3v9"
      },
      "source": [
        "For instance, [1,2] corresponse to \"columbia plus\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2EmJurwiyQx",
        "outputId": "13cc3af4-6bf6-4d1e-d901-7570773e1796"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['columbia plus']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tokenizer.sequences_to_texts([[1,2]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxfq2mM0mDID"
      },
      "source": [
        "Now let's preprocess/tokenize the whole book."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaEWljBZhC5O"
      },
      "outputs": [],
      "source": [
        "# Preprocess the text\n",
        "tokenizer = Tokenizer()\n",
        "corpus = text.lower().split(\"\\n\")\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZD_a3MnKby8",
        "outputId": "f3ee1c9c-a34b-4bcf-d203-f7f33dfc96b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7111"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "total_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3iR7Xs7nerX"
      },
      "source": [
        "## Generating Sequence of N-gram Tokens\n",
        "Language modelling requires a sequence input data, as given a sequence (of words/tokens) the aim is the predict next word/token.\n",
        "\n",
        "## What are N-gram Sequences?\n",
        "An n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words, or base pairs according to the application.\n",
        "In the context of text, an n-gram could be a sequence of n words (word-level n-grams) or characters (character-level n-grams).\n",
        "\n",
        "## Why Use N-gram Sequences?\n",
        "* Context Capturing: N-grams help in capturing the context in the text data. For instance, in a bigram (2-gram) model, each pair of words is considered, which gives the model a sense of word order and context.\n",
        "* Simplifying the Model: By breaking down the text into n-grams, we simplify the problem of predicting the next item in a sequence. Instead of predicting a word from the entire text history, the model only needs to consider the last n-1 items.\n",
        "* Improving Accuracy: N-gram models can significantly improve the accuracy of language models compared to considering each word or character independently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET0i5hsihI7S"
      },
      "outputs": [],
      "source": [
        "# Create input sequences\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQhEwF4zEWhG"
      },
      "source": [
        "In the above output [12, 23],\n",
        " [12, 23, 7],\n",
        " [12, 23, 7, 540],\n",
        " [12, 23, 7, 540, 2505],\n",
        " [12, 23, 7, 540, 2505, 701] and so on represents the ngram phrases generated from the input data. where every integer corresponds to the index of a particular word in the complete vocabulary of words present in the text. For example\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0fEZ6-pntY1",
        "outputId": "75fd424a-947c-41a1-9823-3ecdaf111bb5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[328, 4],\n",
              " [328, 4, 1339],\n",
              " [30, 72],\n",
              " [30, 72, 3117],\n",
              " [256, 2504],\n",
              " [12, 23],\n",
              " [12, 23, 7],\n",
              " [12, 23, 7, 540],\n",
              " [12, 23, 7, 540, 2505],\n",
              " [12, 23, 7, 540, 2505, 701]]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "input_sequences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0q_TjO9EHoS",
        "outputId": "5e9c7cae-a940-4b2d-e728-53ac589cf18f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pride and',\n",
              " 'pride and prejudice',\n",
              " 'by jane',\n",
              " 'by jane austen',\n",
              " 'chapter 1',\n",
              " 'it is',\n",
              " 'it is a',\n",
              " 'it is a truth',\n",
              " 'it is a truth universally',\n",
              " 'it is a truth universally acknowledged']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "tokenizer.sequences_to_texts(input_sequences[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-oVL3p4FqWI"
      },
      "source": [
        "## Padding the Sequences and obtain Variables : Predictors and Target\n",
        "Now that we have generated a data-set which contains sequence of tokens, it is possible that different sequences have different lengths. Before starting training the model, we need to pad the sequences and make their lengths equal. We can use `pad_sequence`` function of Kears for this purpose. To input this data into a learning model, we need to create predictors and label. We will create N-grams sequence as predictors and the next word of the N-gram as label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzVI0WFrhOmw"
      },
      "outputs": [],
      "source": [
        "# Pad sequences and create predictors and label\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "label = ku.to_categorical(label, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmvZn8HDLYih"
      },
      "source": [
        "## LSTMs for Text Generation\n",
        "\n",
        "\n",
        "### What is LSTM?\n",
        "Long Short-Term Memory (LSTM) networks are a special kind of Recurrent Neural Network (RNN) capable of learning long-term dependencies in data sequences.\n",
        "They are particularly effective for tasks involving sequential data, such as time series analysis, speech recognition, and text generation, because they can remember information for long periods, which is a challenge in traditional RNNs.\n",
        "### Why LSTMs Work\n",
        "* LSTMs address the vanishing gradient problem found in traditional RNNs. This problem occurs when gradients become too small to make significant updates to the weights during backpropagation, making it hard for the RNN to learn long-range dependencies.\n",
        "\n",
        "* LSTM units include memory cells that can maintain information in memory for long periods. Key components of these cells are gates: the input gate, the forget gate, and the output gate. These gates regulate the flow of information into and out of the cell, making LSTMs effective at remembering and forgetting information dynamically.\n",
        "\n",
        "### Components of the LSTM Model\n",
        "\n",
        "* Input Layer : Takes the sequence of words as input\n",
        "* LSTM Layer : Computes the output using LSTM units. I have added 100 units in the layer, but this number can be fine tuned later.\n",
        "* Dropout Layer : A regularisation layer which randomly turns-off the activations of some neurons in the LSTM layer. It helps in preventing over fitting. (Optional Layer)\n",
        "* Output Layer : Computes the probability of the best possible next word as output\n",
        "We will run this model for total 100 epoochs but it can be shortened to 10 epochs for a quick demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvPf2wzlhTVG"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "model = Sequential()\n",
        "# TODO: explain what parameters can be changed or finetuned\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(150, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1jueXlME_0Y",
        "outputId": "799367ec-786f-4d25-9fd4-e1d819828acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 18, 100)           711100    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 18, 150)           150600    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 18, 150)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               100400    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 7111)              718211    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1680311 (6.41 MB)\n",
            "Trainable params: 1680311 (6.41 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1zlZNxeHI9O"
      },
      "source": [
        "## Best Practices: Checkpoint\n",
        "Using checkpoints and saving the model is generally considered a very good practice in the field of machine learning and deep learning for several important reasons:\n",
        "\n",
        "* Preventing Data Loss: Training models, especially deep learning models like LSTMs, can be time-consuming and resource-intensive. Checkpoints prevent loss of progress in case of interruptions like power failures or system crashes.\n",
        "\n",
        "* Model Evaluation and Comparison: Saving models at different stages of training (or with different architectures) allows you to compare their performance on the validation set. This helps in selecting the best model for your task.\n",
        "\n",
        "* Early Stopping: Checkpoints can be used in conjunction with early stopping, where training is halted as soon as the model performance begins to degrade on a validation set. This helps prevent overfitting.\n",
        "\n",
        "* Continuing Training: If you decide to train your model further, checkpoints allow you to resume training from a specific point rather than starting over.\n",
        "\n",
        "* Experimentation: Having saved models allows you to experiment with different aspects of your model (like hyperparameters) without losing your previous work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMeMTkbQhXBg",
        "outputId": "5dfce900-21e3-4e3e-f828-5bec16bcb905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 66/100\n",
            "3545/3545 [==============================] - ETA: 0s - loss: 2.8251 - accuracy: 0.3552\n",
            "Epoch 66: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0066.ckpt\n",
            "3545/3545 [==============================] - 39s 11ms/step - loss: 2.8251 - accuracy: 0.3552\n",
            "Epoch 67/100\n",
            "3544/3545 [============================>.] - ETA: 0s - loss: 2.8136 - accuracy: 0.3570\n",
            "Epoch 67: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0067.ckpt\n",
            "3545/3545 [==============================] - 36s 10ms/step - loss: 2.8136 - accuracy: 0.3570\n",
            "Epoch 68/100\n",
            "3542/3545 [============================>.] - ETA: 0s - loss: 2.8036 - accuracy: 0.3567\n",
            "Epoch 68: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0068.ckpt\n",
            "3545/3545 [==============================] - 38s 11ms/step - loss: 2.8036 - accuracy: 0.3568\n",
            "Epoch 69/100\n",
            "3545/3545 [==============================] - ETA: 0s - loss: 2.7933 - accuracy: 0.3613\n",
            "Epoch 69: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0069.ckpt\n",
            "3545/3545 [==============================] - 38s 11ms/step - loss: 2.7933 - accuracy: 0.3613\n",
            "Epoch 70/100\n",
            "3545/3545 [==============================] - ETA: 0s - loss: 2.7792 - accuracy: 0.3622\n",
            "Epoch 70: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0070.ckpt\n",
            "3545/3545 [==============================] - 38s 11ms/step - loss: 2.7792 - accuracy: 0.3622\n",
            "Epoch 71/100\n",
            "3543/3545 [============================>.] - ETA: 0s - loss: 2.7705 - accuracy: 0.3647\n",
            "Epoch 71: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0071.ckpt\n",
            "3545/3545 [==============================] - 39s 11ms/step - loss: 2.7706 - accuracy: 0.3647\n",
            "Epoch 72/100\n",
            "3545/3545 [==============================] - ETA: 0s - loss: 2.7603 - accuracy: 0.3654\n",
            "Epoch 72: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0072.ckpt\n",
            "3545/3545 [==============================] - 38s 11ms/step - loss: 2.7603 - accuracy: 0.3654\n",
            "Epoch 73/100\n",
            "3543/3545 [============================>.] - ETA: 0s - loss: 2.7534 - accuracy: 0.3669\n",
            "Epoch 73: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0073.ckpt\n",
            "3545/3545 [==============================] - 36s 10ms/step - loss: 2.7533 - accuracy: 0.3669\n",
            "Epoch 74/100\n",
            "3541/3545 [============================>.] - ETA: 0s - loss: 2.7385 - accuracy: 0.3698\n",
            "Epoch 74: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0074.ckpt\n",
            "3545/3545 [==============================] - 43s 12ms/step - loss: 2.7390 - accuracy: 0.3698\n",
            "Epoch 75/100\n",
            "3543/3545 [============================>.] - ETA: 0s - loss: 2.7301 - accuracy: 0.3711\n",
            "Epoch 75: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0075.ckpt\n",
            "3545/3545 [==============================] - 38s 11ms/step - loss: 2.7303 - accuracy: 0.3710\n",
            "Epoch 76/100\n",
            "3542/3545 [============================>.] - ETA: 0s - loss: 2.7213 - accuracy: 0.3731\n",
            "Epoch 76: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0076.ckpt\n",
            "3545/3545 [==============================] - 37s 10ms/step - loss: 2.7213 - accuracy: 0.3731\n",
            "Epoch 77/100\n",
            "3541/3545 [============================>.] - ETA: 0s - loss: 2.7121 - accuracy: 0.3738\n",
            "Epoch 77: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0077.ckpt\n",
            "3545/3545 [==============================] - 38s 11ms/step - loss: 2.7123 - accuracy: 0.3737\n",
            "Epoch 78/100\n",
            "3540/3545 [============================>.] - ETA: 0s - loss: 2.7049 - accuracy: 0.3760\n",
            "Epoch 78: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0078.ckpt\n",
            "3545/3545 [==============================] - 36s 10ms/step - loss: 2.7052 - accuracy: 0.3760\n",
            "Epoch 79/100\n",
            "3544/3545 [============================>.] - ETA: 0s - loss: 2.6928 - accuracy: 0.3774\n",
            "Epoch 79: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0079.ckpt\n",
            "3545/3545 [==============================] - 37s 10ms/step - loss: 2.6928 - accuracy: 0.3774\n",
            "Epoch 80/100\n",
            "3545/3545 [==============================] - ETA: 0s - loss: 2.6854 - accuracy: 0.3802\n",
            "Epoch 80: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0080.ckpt\n",
            "3545/3545 [==============================] - 36s 10ms/step - loss: 2.6854 - accuracy: 0.3802\n",
            "Epoch 81/100\n",
            "3544/3545 [============================>.] - ETA: 0s - loss: 2.6796 - accuracy: 0.3798\n",
            "Epoch 81: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0081.ckpt\n",
            "3545/3545 [==============================] - 45s 13ms/step - loss: 2.6795 - accuracy: 0.3798\n",
            "Epoch 82/100\n",
            "3543/3545 [============================>.] - ETA: 0s - loss: 2.6678 - accuracy: 0.3822\n",
            "Epoch 82: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0082.ckpt\n",
            "3545/3545 [==============================] - 39s 11ms/step - loss: 2.6678 - accuracy: 0.3822\n",
            "Epoch 83/100\n",
            "3543/3545 [============================>.] - ETA: 0s - loss: 2.6569 - accuracy: 0.3837\n",
            "Epoch 83: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0083.ckpt\n",
            "3545/3545 [==============================] - 41s 12ms/step - loss: 2.6569 - accuracy: 0.3837\n",
            "Epoch 84/100\n",
            "3540/3545 [============================>.] - ETA: 0s - loss: 2.6490 - accuracy: 0.3862\n",
            "Epoch 84: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0084.ckpt\n",
            "3545/3545 [==============================] - 38s 11ms/step - loss: 2.6491 - accuracy: 0.3862\n",
            "Epoch 85/100\n",
            "3545/3545 [==============================] - ETA: 0s - loss: 2.6406 - accuracy: 0.3865\n",
            "Epoch 85: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0085.ckpt\n",
            "3545/3545 [==============================] - 40s 11ms/step - loss: 2.6406 - accuracy: 0.3865\n",
            "Epoch 86/100\n",
            "3541/3545 [============================>.] - ETA: 0s - loss: 2.6312 - accuracy: 0.3881\n",
            "Epoch 86: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0086.ckpt\n",
            "3545/3545 [==============================] - 41s 12ms/step - loss: 2.6313 - accuracy: 0.3881\n",
            "Epoch 87/100\n",
            "3542/3545 [============================>.] - ETA: 0s - loss: 2.6267 - accuracy: 0.3897\n",
            "Epoch 87: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0087.ckpt\n",
            "3545/3545 [==============================] - 37s 10ms/step - loss: 2.6266 - accuracy: 0.3898\n",
            "Epoch 88/100\n",
            "3540/3545 [============================>.] - ETA: 0s - loss: 2.6215 - accuracy: 0.3905\n",
            "Epoch 88: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0088.ckpt\n",
            "3545/3545 [==============================] - 43s 12ms/step - loss: 2.6216 - accuracy: 0.3904\n",
            "Epoch 89/100\n",
            "3541/3545 [============================>.] - ETA: 0s - loss: 2.6125 - accuracy: 0.3895\n",
            "Epoch 89: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0089.ckpt\n",
            "3545/3545 [==============================] - 36s 10ms/step - loss: 2.6125 - accuracy: 0.3895\n",
            "Epoch 90/100\n",
            "3544/3545 [============================>.] - ETA: 0s - loss: 2.6050 - accuracy: 0.3934\n",
            "Epoch 90: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0090.ckpt\n",
            "3545/3545 [==============================] - 37s 10ms/step - loss: 2.6050 - accuracy: 0.3934\n",
            "Epoch 91/100\n",
            "3544/3545 [============================>.] - ETA: 0s - loss: 2.5947 - accuracy: 0.3964\n",
            "Epoch 91: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0091.ckpt\n",
            "3545/3545 [==============================] - 36s 10ms/step - loss: 2.5947 - accuracy: 0.3964\n",
            "Epoch 92/100\n",
            "3545/3545 [==============================] - ETA: 0s - loss: 2.5909 - accuracy: 0.3946\n",
            "Epoch 92: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0092.ckpt\n",
            "3545/3545 [==============================] - 38s 11ms/step - loss: 2.5909 - accuracy: 0.3946\n",
            "Epoch 93/100\n",
            "3542/3545 [============================>.] - ETA: 0s - loss: 2.5817 - accuracy: 0.3985\n",
            "Epoch 93: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0093.ckpt\n",
            "3545/3545 [==============================] - 36s 10ms/step - loss: 2.5816 - accuracy: 0.3985\n",
            "Epoch 94/100\n",
            "3545/3545 [==============================] - ETA: 0s - loss: 2.5779 - accuracy: 0.3987\n",
            "Epoch 94: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0094.ckpt\n",
            "3545/3545 [==============================] - 37s 11ms/step - loss: 2.5779 - accuracy: 0.3987\n",
            "Epoch 95/100\n",
            "3544/3545 [============================>.] - ETA: 0s - loss: 2.5654 - accuracy: 0.4003\n",
            "Epoch 95: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0095.ckpt\n",
            "3545/3545 [==============================] - 36s 10ms/step - loss: 2.5654 - accuracy: 0.4003\n",
            "Epoch 96/100\n",
            "3541/3545 [============================>.] - ETA: 0s - loss: 2.5609 - accuracy: 0.4020\n",
            "Epoch 96: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0096.ckpt\n",
            "3545/3545 [==============================] - 37s 10ms/step - loss: 2.5611 - accuracy: 0.4019\n",
            "Epoch 97/100\n",
            "3544/3545 [============================>.] - ETA: 0s - loss: 2.5516 - accuracy: 0.4030\n",
            "Epoch 97: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0097.ckpt\n",
            "3545/3545 [==============================] - 37s 11ms/step - loss: 2.5515 - accuracy: 0.4030\n",
            "Epoch 98/100\n",
            "3543/3545 [============================>.] - ETA: 0s - loss: 2.5445 - accuracy: 0.4050\n",
            "Epoch 98: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0098.ckpt\n",
            "3545/3545 [==============================] - 39s 11ms/step - loss: 2.5447 - accuracy: 0.4050\n",
            "Epoch 99/100\n",
            "3545/3545 [==============================] - ETA: 0s - loss: 2.5374 - accuracy: 0.4043\n",
            "Epoch 99: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0099.ckpt\n",
            "3545/3545 [==============================] - 36s 10ms/step - loss: 2.5374 - accuracy: 0.4043\n",
            "Epoch 100/100\n",
            "3543/3545 [============================>.] - ETA: 0s - loss: 2.5323 - accuracy: 0.4069\n",
            "Epoch 100: saving model to /content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-0100.ckpt\n",
            "3545/3545 [==============================] - 37s 11ms/step - loss: 2.5325 - accuracy: 0.4068\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7df2c039e410>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# path to checkpoint\n",
        "checkpoint_path = \"/content/drive/MyDrive/GenAI-BA (Prof Hardeep Johar)/LSTM Text Generation/training_1/checkpoint-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                              save_weights_only=True,\n",
        "                              verbose=1)\n",
        "# early stopping helps prevent overfitting\n",
        "early_stop = EarlyStopping(monitor='loss', patience=5, min_delta=0.0001)\n",
        "\n",
        "# Train the model\n",
        "# If we have a saved checkpoint, load the checkpoint\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "    model.load_weights(latest_checkpoint)\n",
        "\n",
        "    # Extracting the epoch number from the checkpoint file name\n",
        "    latest_epoch = int(latest_checkpoint.split('-')[-1][:4])\n",
        "else:\n",
        "    latest_epoch = 0\n",
        "\n",
        "# Train the model\n",
        "model.fit(predictors, label, epochs=100, initial_epoch=latest_epoch, verbose=1, callbacks=[early_stop, cp_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The fun part: write like Jane Austen\n",
        "Now that we have finished training our neural network, we can test how it performs and whether it can write like Jane Austen. I certainly hope so.\n",
        "\n",
        "In the below `generate_text` function, we predict the next word based on the input words (or seed text). We first tokenize the seed text, pad the sequences and pass into the trained model to get predicted word. The multiple predicted words can be appended together to get predicted sequence."
      ],
      "metadata": {
        "id": "nbtjbeIlsVXu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeFkkorXhbhX"
      },
      "outputs": [],
      "source": [
        "# Function to generate text\n",
        "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted = model.predict(token_list, verbose=0)\n",
        "        predicted_claess=np.argmax(predicted,axis=1)\n",
        "        output_word = \"\"\n",
        "\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_claess:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Moment of truth!\n",
        "\n",
        "It's fun to explore the text our model generates.\n",
        "\n",
        "Some part of the text generated makes sense, especially if the seed word has been seen multiple times in our text:\n",
        "\n",
        "Marry as soon as possible... (LOL)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BPHUUMThtUjp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9qi5O5xhgP6",
        "outputId": "55e93962-8632-4209-82e9-b9a9cc489650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jane and the two gentlemen turning back and down the chief\n",
            "Elizabeth was forced to know whether she had formerly been ago\n",
            "marry as soon as possible but i have heard the spot\n"
          ]
        }
      ],
      "source": [
        "# Generate text\n",
        "print(generate_text(\"Jane\", 10, model, max_sequence_len))\n",
        "print(generate_text(\"Elizabeth\", 10, model, max_sequence_len))\n",
        "print(generate_text(\"marry\", 10, model, max_sequence_len))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, when the seed word is never seen, our model start to generate meaningless text"
      ],
      "metadata": {
        "id": "Sbi98j5yu2ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(\"Columbia\", 10, model, max_sequence_len))\n",
        "print(generate_text(\"Plus\", 10, model, max_sequence_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlYkVHJ2uQ62",
        "outputId": "b5fd1f6a-5ccc-4137-c2ab-ac88bb00f089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columbia situation and return to the house amidst the nods and\n",
            "Plus situation and return to the house amidst the nods and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the model for future use"
      ],
      "metadata": {
        "id": "Vvq8H3lfvI2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model as a `.keras` zip archive so we can use it later\n",
        "model.save('janeausten.keras')"
      ],
      "metadata": {
        "id": "zGO93n66U6rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next time you run this code, you can directly restore the model\n",
        "new_model = tf.keras.models.load_model('janeausten.keras')\n",
        "\n",
        "# Show the model architecture\n",
        "new_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx5l17N8VExl",
        "outputId": "96b8ce2b-7725-4694-8914-6c9296c29a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 18, 100)           711100    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 18, 150)           150600    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 18, 150)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               100400    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 7111)              718211    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1680311 (6.41 MB)\n",
            "Trainable params: 1680311 (6.41 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the restored model by generating text\n",
        "print(generate_text(\"Jane\", 20, model, max_sequence_len))\n",
        "print(generate_text(\"Elizabeth\", 20, model, max_sequence_len))\n",
        "print(generate_text(\"marry\", 20, model, max_sequence_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I3fx7uTVco9",
        "outputId": "4cfe3f58-b018-4a2a-c757-6cab6b4f9681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jane and the two gentlemen turning back and down the chief of their meeting with mr collins and i dont say\n",
            "Elizabeth was forced to know whether she had formerly been ago she had not yet been calculated against ten as i\n",
            "marry as soon as possible but i have heard the spot young ladies retired to express her father lodge by the\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}